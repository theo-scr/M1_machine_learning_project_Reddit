{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(2-1)_features_text_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYC9eHdOc6yi"
      },
      "source": [
        "<strong>Date :</strong> Créé le 03 Avril 2021| Mis à jour le 09 Avril 2021 </strong>\n",
        "\n",
        "<strong>Compétition Kaggle - Team Théo\n",
        "    \n",
        "@auteur : </strong>Théo SACCAREAU & Théo VEDIS\n",
        "\n",
        "<strong>(2-1)_features_text_1\n",
        "      \n",
        "Description :</strong> A travers ce Notebook, nous détaillerons notre méthode pour créer les différentes features basées sur le texte.\n",
        "\n",
        "Temps d'exécution du Notebook : environ  1h10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imAEsGzZGyz-"
      },
      "source": [
        "# Installation / Téléchargement / Importation des librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymWMQzYb97pb"
      },
      "source": [
        "!pip install spacy \n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcvnUgP4Gyz4"
      },
      "source": [
        "# Librairies usuelles\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm \n",
        "\n",
        "# Librairies pour le texte\n",
        "# (1) NLTK\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# (2) Spacy \n",
        "import spacy\n",
        "import en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlUNxsXdGMpP"
      },
      "source": [
        "# Chemin "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY896iIwGy0G"
      },
      "source": [
        "# Chemin relatif vers le dossier \"data\" (inutile de le changer).\n",
        "pathFile = \"../data/\" "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaPBGrY2AciM"
      },
      "source": [
        "# Chargement des données d'entrée"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAwAmPvxAI3w"
      },
      "source": [
        "# Chargement du fichier contenant le DataFrame retourné par le Notebook précédent.\n",
        "# Temps d'exécution : 1min30.\n",
        "df = pd.read_json(pathFile + \"df_clean.json\") "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErLPwnwZoJCl"
      },
      "source": [
        "# (1) Traitement du texte. \n",
        "Dans cette première partie, nous effectuerons un nettoyage du texte : tokenisation, lemmentisation, suppression des mots-vides, etc. Nous en profiterons pour créer quelques features qui nous semblent être utiles pour la suite. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cp6MK5tpIVD"
      },
      "source": [
        "## 1-1 Commentaires \"robots\" \n",
        "Pour commencer, nous allons vérifier que le contenu du commentaire n'est pas un commentaire \"robot\". Pour comprendre ce qu'on appelle \"commentaire robot\", il faut observer les commentaires qui reviennent le plus souvent : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWIziXI4JW_M",
        "outputId": "666c1ffb-0ba5-4e08-fd7a-ce9f69f5db56"
      },
      "source": [
        "df[\"body\"].value_counts()[:10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                284302\n",
              "**Attention!** Please keep in mind that the OP of this thread has chosen to mark this post with the **[Serious] replies only** tag, therefore [any replies that are jokes, puns, off-topic, or are otherwise non-contributory will be removed](http://www.reddit.com/r/AskReddit/wiki/index#wiki_--.5Bserious.5D_tags--).  If you see others posting comments that violate this tag, please report them to the mods!   Thanks for your cooperation and enjoy the discussion!   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/AskReddit) if you have any questions or concerns.*                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             9956\n",
              "**PLEASE READ THIS MESSAGE IN ITS ENTIRETY BEFORE TAKING ACTION.**  It looks like you failed to ask a question in your title. Double check everything, including punctuation, and try posting again.   * **If you're wanting to tell a story or explain something, please try /r/Self.**  * **If you're needing advice of any kind, try /r/advice or /r/needadvice**  * **If you're looking for a fact based answer, try /r/answers**  * **If you're looking for something that has slipped your mind, try /r/TipOfMyTongue.**  Anything else not covered try one of the subreddits mentioned in our sidebar that better suits your needs.  If you need more clarification, or feel this was an error, please contact the mods and provide a link to this post, thanks!   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/AskReddit) if you have any questions or concerns.*                                                                                                                                                                                                                  5837\n",
              "**Attention!** Please keep in mind that the OP of this thread has chosen to mark this post with the **[Serious] replies only** tag, therefore [any replies that are jokes, puns, off-topic, or are otherwise non-contributory will be removed](http://www.reddit.com/r/AskReddit/wiki/index#wiki_--.5Bserious.5D_tags--).  If you don't fall within the scope the question is directed to, please do not reply to the question as your comment will be removed. If the question is Bakers of reddit... and you're not a baker, your comment will be removed. All top-level replies need to be from someone who is in the group the question was asked to.  If you see others posting comments that violate this tag, please report them to the mods!   Thanks for your cooperation and enjoy the discussion!   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/AskReddit) if you have any questions or concerns.*                                                                                                                                                                             3385\n",
              "This submission has been automatically removed because you did not include a question mark (?) in your title. Reddit does not allow post titles to be edited, so if you would like, you can post the question again.  Please write your title in proper question format, and include a question mark, thank you.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/AskReddit) if you have any questions or concerns.*                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          2831\n",
              "**PLEASE READ THIS MESSAGE IN ITS ENTIRETY BEFORE TAKING ACTION.**  Hi there, your post has been removed for one of the following reasons:   * [Rule 1:](http://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_1-) You may not have a story in your title. You must post a clear and direct question, and only the question, in your title.Questions about you or your specific personal situation are not appropriate for Askreddit. Additionally, any answers to the question, including your own, should go in the comments as a reply to your own post.  * [Rule 2:](http://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_2-) Questions about you or your specific personal situation are not appropriate for Askreddit. Try /r/Self, /r/Advice, or /r/NeedAdvice. If you have questions regarding reddit try /r/help.  If you have any queries or concerns, please feel free to [message the mods.](http://www.reddit.com/message/compose?to=%2Fr%2FAskReddit) Thank you.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/AskReddit) if you have any questions or concerns.*      2058\n",
              ":(                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                1594\n",
              "Hello OP,   Your post has been removed as a result of **[Rule 3 (Click for more info)](https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_3-)**  &gt;Askreddit is for open-ended discussion questions. Questions with a single correct answer, that can be researched elsewhere or provide a limited scope for discussion (yes/no, DAE, polls and surveys, etc.) are not appropriate.  If you feel this message was sent by mistake, please feel free to contact the mods [here](https://www.reddit.com/message/compose?to=%2Fr%2FAskReddit).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               1020\n",
              "Every account on reddit is a bot except you.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       867\n",
              "Yes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                821\n",
              "Name: body, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tGI276upYXk"
      },
      "source": [
        "En effet, en faisant ce `value_counts` nous nous sommes aperçu qu'il y avait quelques commentaires qui reviennent assez souvent et qui s'apparentent à des spams/commantaires bots. <br> \n",
        "Nous estimons alors qu'il peut être pertinent d'avoir une feature indiquant si oui ou non le commenataire est un \"commentaire robot\". Selon nous, si c'est le cas, le score ne sera pas élevé. \n",
        "\n",
        "Nous avons donc décider de considérer les commentaires qui reviennent plus de 50 fois et dont la taille est supérieure à 20 caractères (on fixe également une limite de caractère pour que les commentaires \"Yes\", \"Thanks!\", \"No\", ...,  qui reviennent souvent ne soient pas considérés comme des commentaires robots). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CLnjhasL036",
        "outputId": "6d4e4d8a-dad6-4e9e-9ab0-69c01c04e081"
      },
      "source": [
        "# Seuils \n",
        "threshold_freq = 50\n",
        "threshold_length = 20\n",
        "\n",
        "# Value_counts\n",
        "vc = df[\"body\"].value_counts()\n",
        "\n",
        "# Ensemble contenant les bodys considérés comme SPAM \n",
        "bot_comment = set([comment for comment, val in zip(\n",
        "    tqdm(vc.index), vc.values) if val > threshold_freq and len(comment) > threshold_length])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3675455/3675455 [00:03<00:00, 920707.56it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fi_Se8Unz0A"
      },
      "source": [
        "# On parcout la liste des contenus, si le contenu est dans l'ensemble précédent\n",
        "# alors on met comme valeur 1, sinon 0.  \n",
        "df['bot_comment'] = df['body'].apply(lambda x : 1 if x in bot_comment else 0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2giRqhHnz96",
        "outputId": "a4f4d314-5a87-4772-ba46-4662ee622b32"
      },
      "source": [
        "sum(df['bot_comment'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30960"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6peIxxPugFl"
      },
      "source": [
        "Plus de 30 000, commentaires sont considérés comme des SPAM. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anelv9WPtf1x"
      },
      "source": [
        "## 1-2 Taille du commentaire avant NLP\n",
        "Nous estimons que la taille du commentaire est importante. En effet, un commentaire très court (quelques caractères seulement), aura surement un score faible alors qu'un commentaire avec une phrase complète aura plus de chance d'avoir un bon score. Toutefois, nous pensons que la taille du commentaire ne doit pas être non plus trop grande car sinon les utilisateurs n'auront pas forcément envie de lire un gros paragraphe, et donc le score ne sera pas très élevé. \n",
        "\n",
        "Nous distinguons plusieurs types de taille : \n",
        "- la taille (en nombre de caractères) avant et après la suppression des stopwords (mots vides) et nettoyage du texte. Effectivement, si une phrase avait une longueur assez grande avant ce traitement et qu'après sa longueur devient très faible, alors forcément on se dit que la richesse du vocabulaire était pauvre (beaucoup de mots vides, etc). \n",
        "- la taille (nombre de mots) après le nettoyage du texte. \n",
        "- nombre de mots vides."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4BGvrn4W0_d",
        "outputId": "95119ce7-854a-42c2-efd1-cef6b51e7d0c"
      },
      "source": [
        "# Taille du body en nombre de caractères\n",
        "df['length_comment_chars_before_NLP'] = [len(x) for x in tqdm(df['body'])]\n",
        "df['length_comment_chars_before_NLP'][:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4234970/4234970 [00:02<00:00, 1697727.26it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    119\n",
              "1     48\n",
              "2      4\n",
              "3     54\n",
              "4    241\n",
              "5     22\n",
              "6    178\n",
              "7      7\n",
              "8     70\n",
              "9    171\n",
              "Name: length_comment_chars_before_NLP, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h3TU9l6-67x"
      },
      "source": [
        "# Avec ce \"tokenizer\", on ne prend en compte que les caractères alpha-numérique, \n",
        "# la ponctuation est donc exclue.\n",
        "tokenizer = nltk.RegexpTokenizer(r'\\w+')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDqiQ0gIn0Bj",
        "outputId": "7c8448f0-c2c2-49b3-bd8f-56329842c98a"
      },
      "source": [
        "# Taille du body en nombre de mots avant le traitement du texte.\n",
        "length_comment_words_before_NLP = [\n",
        "    len(tokenizer.tokenize(x)) for x in tqdm(df['body'])]\n",
        "    \n",
        "length_comment_words_before_NLP[:10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4234970/4234970 [00:31<00:00, 133626.49it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[22, 9, 1, 15, 44, 3, 40, 2, 12, 32]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kybsfIUNXG4X"
      },
      "source": [
        "## 1-3 NLP "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSnPQS5rXLg_"
      },
      "source": [
        "### 1-3-1 Suppression des stopwords\n",
        "L'objectif de cette étape est d'éliminer les \"stopwords\", c'est-à-dire les mots beaucoup utilisés mais peu informatifs et qui allourdissent notre jeux de données."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRcRq1ran0er",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c968f7-dcae-44f2-ec9e-fef540f60b68"
      },
      "source": [
        "# Ensemble de mots-vides en anglais.\n",
        "stop_words = set(tuple(nltk.corpus.stopwords.words('english')))\n",
        "\n",
        "# On supprime les mots vides\n",
        "df['bodyNLP1'] = [\" \".join([i for i in tokenizer.tokenize(\n",
        "    x.lower()) if not i in stop_words and len(i) >= 4]) for x in tqdm(df['body'])]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4234970/4234970 [00:55<00:00, 75929.24it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz0LDrk0n0hs",
        "outputId": "e2ca3040-39cc-4eac-8ae1-dc591a7f11f1"
      },
      "source": [
        "# Body avant suppression des mots-vides.\n",
        "df['body'][:5]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    No one has a European accent either  because i...\n",
              "1     That the kid ..reminds me of Kevin.   so sad :-(\n",
              "2                                                 NSFL\n",
              "3    I'm a guy and I had no idea this was a thing g...\n",
              "4    Mid twenties male rocking skinny jeans/pants, ...\n",
              "Name: body, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd8l9dPBbbjI",
        "outputId": "1e4ccf7f-2b38-45c8-a915-28d0bbcec90e"
      },
      "source": [
        "# Body après suppression des mots-vides.\n",
        "df['bodyNLP1'][:5]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    european accent either exist accents europe eu...\n",
              "1                                        reminds kevin\n",
              "2                                                 nsfl\n",
              "3                                      idea thing guys\n",
              "4    twenties male rocking skinny jeans pants style...\n",
              "Name: bodyNLP1, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DQtpAvnfS-M"
      },
      "source": [
        "### 1-3-2 Lemmentisation ou racinisation  \n",
        "C'est la dernière étape pour le prétraitement.\n",
        "\n",
        "Le processus de « lemmatisation » consiste à représenter les mots sous leur forme canonique. Par exemple pour un verbe, ce sera son infinitif. Pour un nom, son masculin singulier. L'idée étant encore une fois de ne conserver que le sens des mots utilisés dans le corpus.\n",
        "\n",
        "Il existe un autre processus qui exerce une fonction similaire qui s'appelle la racinisation (ou stemming en anglais). Cela consiste à ne conserver que la racine des mots étudiés. L'idée étant de supprimer les suffixes, préfixes et autres des mots afin de ne conserver que leur origine. \n",
        "\n",
        "Nous utiliserons ici la lemmentisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDSLCYBfP4l8",
        "outputId": "5d826f98-e6d3-44f5-e98b-d06a8de4f218"
      },
      "source": [
        "# Création d'un lemmatiseur\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# Application de ce lemmatiseur au body après suppression des mots vides (NLP1)\n",
        "df['bodyNLP2'] = pd.Series([\" \".join([wnl.lemmatize(\n",
        "    words) for words in tokenizer.tokenize(x)]) for x in tqdm(df['bodyNLP1'])])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4234970/4234970 [04:32<00:00, 15548.93it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3qHh8JUcTe8",
        "outputId": "3461ce38-bc8e-4b0c-aab4-2b106f653b04"
      },
      "source": [
        "# Résultat après lemmentisation \n",
        "df['bodyNLP2'][:5]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    european accent either exist accent europe eur...\n",
              "1                                        reminds kevin\n",
              "2                                                 nsfl\n",
              "3                                       idea thing guy\n",
              "4    twenty male rocking skinny jean pant styled ha...\n",
              "Name: bodyNLP2, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL2y_PHWoK2Q"
      },
      "source": [
        "## 1-4 Taille du commentaire après NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGGed0Z8cTnj",
        "outputId": "944812e9-24b7-4b77-ec6b-580a34fabf17"
      },
      "source": [
        "# Taille du commentaire après suppression des mots-vides (NLP1) (caractères)\n",
        "df['length_comment_chars_after_NLP'] = [len(x) for x in tqdm(df['bodyNLP1'])]\n",
        "df['length_comment_chars_after_NLP'][:10]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4234970/4234970 [00:02<00:00, 1762469.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     59\n",
              "1     13\n",
              "2      4\n",
              "3     15\n",
              "4    164\n",
              "5     22\n",
              "6     82\n",
              "7      0\n",
              "8     50\n",
              "9    122\n",
              "Name: length_comment_chars_after_NLP, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w3nDAKbuZE3",
        "outputId": "00ac941d-dbd9-48e5-c471-55790ab059f8"
      },
      "source": [
        "# Taille du commentaire après suppression des mots-vides (mots)\n",
        "df['length_comment_words'] = [len(tokenizer.tokenize(\n",
        "    comment_after)) for comment_after in tqdm(df['bodyNLP1'])]\n",
        "    \n",
        "df['length_comment_words'][:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4234970/4234970 [00:16<00:00, 255522.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     8\n",
              "1     2\n",
              "2     1\n",
              "3     3\n",
              "4    24\n",
              "5     3\n",
              "6    13\n",
              "7     0\n",
              "8     7\n",
              "9    18\n",
              "Name: length_comment_words, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWhqdxGGcTsi",
        "outputId": "e7be79c8-2b6d-479c-ef57-ceda66fdbf24"
      },
      "source": [
        "# On en déduit le nombre de mots vides (en faisant la soustraction du nombre de \n",
        "# mots avant et après le traitement)\n",
        "df['nb_stopwords'] = [nb_before - nb_after for nb_before, nb_after in zip(\n",
        "    tqdm(length_comment_words_before_NLP), df['length_comment_words'])]\n",
        "    \n",
        "df['nb_stopwords'][:10]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4234970/4234970 [00:01<00:00, 2168763.17it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    14\n",
              "1     7\n",
              "2     0\n",
              "3    12\n",
              "4    20\n",
              "5     0\n",
              "6    27\n",
              "7     2\n",
              "8     5\n",
              "9    14\n",
              "Name: nb_stopwords, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60aItt-gcTxX",
        "outputId": "86b0456d-8dbf-40c9-843a-6af3fd03cb6f"
      },
      "source": [
        "# Nombre de mots avant traitement du texte\n",
        "sum(length_comment_words_before_NLP)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114581371"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGuIOgS_rZU_",
        "outputId": "26b3b9e4-3cbd-460f-ee8a-fcb2d1517a1a"
      },
      "source": [
        "# Nombre de mots après traitement du texte\n",
        "sum(df['length_comment_words'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48111665"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dogzjG7Hrsoa"
      },
      "source": [
        "Avant le traitement, nous avions plus de 114 milliards de mots, après le traitement il n'y en a plus \"que\" 64 milliards. Près de 50% des mots étaients donc des mots vides ! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFDqizZxpBWY"
      },
      "source": [
        "# (2) Analyse d'opinions / de sentiments \n",
        "Dans cette deuxième partie, nous avons fait le choix d'effectuer une analyse de sentiments sur les commentaires. Cette analyse a permis de déterminer pour chaque commentaire s'il était plutôt \"positif\", \"neutre\", \"negatif\". Nous estimons que cette information peut être utile pour prédire le score d'un commentaire. <br> \n",
        "En effet, un commentaire \"positif\" aura selon nous plus de chance d'être bien noté. Toutefois, un commentaire \"négatif\" peut lui aussi bien être noté s'il répond à un sujet/post polémique auquel la majorité des utilisateurs de Reddit ne sont pas d'accord. Enfin, un commentaire \"neutre\" provoque selon nous moins de réaction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86ldJrnawi94"
      },
      "source": [
        "def sentiment_cat(res):\n",
        "    \"\"\"\n",
        "    Fonction qui permet à partir des résultats de l'analyseur de sentiments \n",
        "    de déterminer si le commentaire est positif, négatif ou neutre.\n",
        "    Paramètre : \n",
        "        - res (float compris entre -1 et 1) : résultat de l'analyseur\n",
        "    \n",
        "    Sortie : \n",
        "        - sent (str) : chaine indiquant si le commentaire est \"positif\", \n",
        "        \"négatif\", ou \"neutre\". \n",
        "    \"\"\"\n",
        "\n",
        "    if (res > 0):\n",
        "        sent = 'pos'\n",
        "    elif (res == 0):\n",
        "        sent = 'neu'\n",
        "    elif (res <0):\n",
        "        sent = 'neg'\n",
        "\n",
        "    return sent"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLA0RcQsqSmj",
        "outputId": "a92fb089-5d2d-49a7-d632-b93d791a7e25"
      },
      "source": [
        "# Analyseur de sentiment \n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Pour chaque contenu de commentaire, on applique l'analyseur puis on fait \n",
        "# appel à la fonction précédente pour déterminer son sentiment. \n",
        "df['sentiment'] = [sentiment_cat(sia.polarity_scores(\n",
        "    x)['compound']) for x in tqdm(df['body'])]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4234970/4234970 [27:34<00:00, 2559.36it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8iVekcQzIxl"
      },
      "source": [
        "(!) Attention (!) Pour l'analyse de sentiments, il est mieux d'utiliser le corpus non nettoyé. En effet, les signes de ponctuations (!!), les émoticones ( :), :D, etc) sont des éléments que SIA prend en compte. Ils permettent donc d'améliorer les résultats.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm-eVrlIcURO",
        "outputId": "4286b196-bc58-45e2-c3ee-72d80beede16"
      },
      "source": [
        "# Résultat \n",
        "df['sentiment'][:10]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    neg\n",
              "1    neg\n",
              "2    neu\n",
              "3    neg\n",
              "4    pos\n",
              "5    pos\n",
              "6    pos\n",
              "7    neu\n",
              "8    pos\n",
              "9    neg\n",
              "Name: sentiment, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM64fyqZ5dmV"
      },
      "source": [
        "A partir de ces résultats, on en déduit une nouvelle feature : les sentiments des enfants. <br> \n",
        "En considérant qu'un commentaire positif rapporte +1, qu'un commentaire négatif rapporte -1 et qu'un commentaire neutre rapporte 0, on fait la somme de tous les sentiments des enfants d'un commentaire. Cette somme peut être utile pour prédire un score. En effet, selon nous, plus cette somme est négative, plus les utilisateurs qui réponde à ce commentaire sont en désaccord et donc plus le commentaire aura tendance à avoir un score faible. A l'inverse, un commentaire où ses enfants sont majoritairement positifs, aura un score plus élevé. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGMo6na3TNZU",
        "outputId": "1792c295-74e7-46d1-ca23-4a64bec4457c"
      },
      "source": [
        "# On crée un dictionnaire qui contiendra la liste des sentiments de \n",
        "# leurs enfants.\n",
        "dict_fils_sentiment = dict()\n",
        "\n",
        "# On parcourt le DataFrame en récupérant les identifiants des parents \n",
        "# et les sentiments des commentaires. \n",
        "for parent, sentiment in zip(tqdm(df['parent_id']), df['sentiment']):\n",
        "\n",
        "    # Si le parent est déjà dans le dictionnaire, on raajoute le sentiment \n",
        "    # de son nouvel enfant. \n",
        "    if parent in dict_fils_sentiment:\n",
        "        dict_fils_sentiment[parent] = dict_fils_sentiment[parent] + [sentiment]\n",
        "    \n",
        "    # Sinon, c'est son premier, on créer donc une liste d'un élément contenant \n",
        "    # le sentiment de son premier enfant \n",
        "    else : \n",
        "        dict_fils_sentiment[parent] = [sentiment]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4234970/4234970 [00:15<00:00, 280766.16it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s50R2eqadVRp"
      },
      "source": [
        "def sentiment_child(liste):\n",
        "    \"\"\"\n",
        "    Fonction qui traduit la liste des sentiments des enfants en somme pour \n",
        "    déterminer si les enfats sont plutôt positifs (somme > 0), neutre (somme =0)\n",
        "    ou négative (somme < 0). \n",
        "    Paramètre : \n",
        "        - liste (liste) : liste des sentiments des enfants \n",
        "    \n",
        "    Sortie : \n",
        "        - somme (int) : somme selon les sentiments des enfants.  \n",
        "    \"\"\"\n",
        "\n",
        "    # Value_counts qui permet de connaitre la proportion de 'pos', 'neg' et 'neu'\n",
        "    vc = pd.Series(liste).value_counts()\n",
        "\n",
        "    # Dictionnaire permettant de convertir sentiment en entier\n",
        "    dic = {\n",
        "        'neu' : 0, \n",
        "        'pos' : 1, \n",
        "        'neg' : -1\n",
        "    }\n",
        "\n",
        "    return sum([dic[i] * val for i, val in zip(vc.index, vc.values)])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcJB5RnadVa7",
        "outputId": "0648402a-035b-4b8e-dcf8-4805032014b3"
      },
      "source": [
        "# On applique la fonction précédent à chaque commentaire\n",
        "df['sentiment_child'] = [sentiment_child(\n",
        "    dict_fils_sentiment[comment]) if comment in dict_fils_sentiment else 0 for comment in tqdm(df['name'])]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4234970/4234970 [23:36<00:00, 2990.09it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJiV6FCms-j-",
        "outputId": "5e86771b-9b46-4daa-eb45-2fb84d8a80e6"
      },
      "source": [
        "df['sentiment_child'].value_counts()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 0      3341576\n",
              " 1       471415\n",
              "-1       313190\n",
              " 2        44331\n",
              "-2        26829\n",
              "         ...   \n",
              " 132          1\n",
              " 140          1\n",
              " 141          1\n",
              " 144          1\n",
              " 96           1\n",
              "Name: sentiment_child, Length: 174, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hetryfft7wdB"
      },
      "source": [
        "Il y a une grande majorité de commantaire (3,34 millions) qui ont des enfants sans réaction particulière (somme =0). Cela s'explique par le fait que beaucoup de commentaire ne reçoivent pas de réponse (cf Notebooks suivants). <br>\n",
        "Ensuite, le nombre de commantaires ayant des enfants positifs est globalement supérieur au nombre de commantaires ayant des enfants négatifs. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9LPilg7irHH"
      },
      "source": [
        "# (3) Similarité avec le topic \n",
        "La dernière partie pour les features basées sur le texte porte sur la similarité entre le contenu du sujet/topic et le contenu du commentaire. Plus la similitude est proche, plus le commentaire traite du même sujet que le topic concerné. Ainsi, nous estimons que si la similitude est importante, le score du commentaire a de bonnes chances d'avoir un score élevé. <br> \n",
        "Nous nous sommes basé sur la métrique du cosinus. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTypuJhrkr-X"
      },
      "source": [
        "def embeddings_similarity(vector_a, vector_b):\n",
        "    \"\"\"\n",
        "    Fonction qui calcule la similarité entre deux contenus textuels en se\n",
        "    basant sur la méritque cosinus. \n",
        "\n",
        "    Paramètres : \n",
        "        - vector_a (list) : vecteur représentant le 1er contenu textuel \n",
        "        - vector_b (list) : vecteur représentant le 2e contenu textuel \n",
        "\n",
        "    Sortie : \n",
        "        - simi (float) : nombre décimal compris entre 0 et 1 indquant la \n",
        "        similarité entre les deux vectors (plus il est proche de 1 plus la \n",
        "        similarité est forte)\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Calcul de la similarité avec la formule du cosinus\n",
        "        simi = np.dot(vec_1, vec_2) / \\\n",
        "            (np.linalg.norm(vec_1)*np.linalg.norm(vec_2))\n",
        "    except:\n",
        "        # S'il y a une erreur dans le calcul (notamment quand l'un des vecteurs\n",
        "        # est nul parce que pas de contenu dans le body), on donne la valeur 0.\n",
        "        simi = 0\n",
        "\n",
        "    return simi"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6a3xnQy0byF"
      },
      "source": [
        "# Dans un premier temps, on récupère les informations scrapées sur les topics.\n",
        "# En effet, dans notre DataFrame, nous ne disposions pas des contenus des posts\n",
        "# nous les avons donc scrappés sur Reddit. \n",
        "# Temps exécution : 50s\n",
        "df2 = pd.read_json(pathFile + \"data_post.json\").T "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCiIWfL81ROc"
      },
      "source": [
        "# Embeddins (version anglais) permettant de traduire un contenu textuel en\n",
        "# vecteur.\n",
        "embeddings = spacy.load('en_core_web_sm')\n",
        "\n",
        "# On applique le même nettoyage au contenu des posts que celui effectué pour\n",
        "# le contenu des commentaires (tokenisation suppresion mots vides et lemmentisation)\n",
        "body_topics = df2['title'].apply(lambda x: \" \".join(\n",
        "    [i for i in tokenizer.tokenize(x.lower()) if not i in stop_words and len(i) >= 4]))\n",
        "body_topics = [\" \".join([wnl.lemmatize(words)\n",
        "                         for words in tokenizer.tokenize(x)]) for x in tqdm(body_topics)]\n",
        "\n",
        "# Une fois nettoyés, on transforme les contenus des posts en vecteur.\n",
        "#body_topics = [embeddings(topic).vector for topic in tqdm(body_topics)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi91VQxc0oL7"
      },
      "source": [
        "# On crée un dictionnaire qui associe à chaque topic leur vecteur représentant \n",
        "# leur contenu. \n",
        "dict_topic_body = dict([(topic, body) for topic, body in zip(tqdm(df2.index), body_topics)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnqd0lmfksL0"
      },
      "source": [
        "# On applique la fonction permettant de calculer la similarité entre le contenu\n",
        "# d'un commentaire et le contenu du post auquel il répond.\n",
        "\n",
        "# df['cosSimWithTopic'] = [embeddings_similarity(embeddings(body).vector, dict_topic_body[topic])\n",
        "#                         if topic in dict_topic_body else 0 for body, topic in zip(tqdm(df['bodyNLP2']), df['link_id'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Idm7eTHBHm3"
      },
      "source": [
        "Etant donné la longueur du DataFrame (4 millions de commentaires), cette fonction prend un temps trsè très important (près de 24h). Nous avons fait le choix, par soucis de temps, de ne pas utiliser cette fonction (bien que nous estimons après coup que cette feature aurait pu nous être d'une précieuse aide pour le modèle). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTQbGfFn0R71"
      },
      "source": [
        "# Sauvegarde \n",
        "La partie sur le texte n'est pas tout à fait fini, nous souhaitons réaliser également une analyse topicale. Cependant, nous ne pouvons pas la réaliser dans ce même Notebook car la RAM ne supportait pas toutes les opérations. Nous enregirtons donc les résultats dans un fichier et nous procéderons donc à l'analyse topicale dans le Notebook suivant. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kd9o2WP0iOE"
      },
      "source": [
        "Pour ne pas surchager inutilement les fichiers de sauvegarde, nous supprimons les colonnes `body`, `bodyNLP1` et `bodyNLP2`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UAQURFx1FhM"
      },
      "source": [
        "# On stocke le contenu nettoyé à part car on l'utilisera pour l'analyse topicale\n",
        "data_LDA = df['bodyNLP2']\n",
        "\n",
        "# Colonnes à supprimer \n",
        "columns_to_delete = ['body', 'bodyNLP1', 'bodyNLP2']\n",
        "\n",
        "df = df.drop(columns=columns_to_delete)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KETgt61y0OZN"
      },
      "source": [
        "# Sauvegarde DataFrame dans un fichier \n",
        "df.to_json(pathFile + \"df_features_text.json\")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYt7AaZ7-m-W"
      },
      "source": [
        "# Sauvegarde du contenu des commentaires nettoyé\n",
        "data_LDA.to_csv(pathFile + \"data_LDA.csv\", index=False)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqPKLhC7-xO2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}